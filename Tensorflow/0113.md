1. 일단 학습 데이터 수가 많아야 해요!
2. 필요없는 feature들은 학습에서 제외
3. 중복되는 feature들은 단일화 시켜야함
4. 학습하는 과정에서 overfitting을 피할 수 있음 

> `dropout`이라고 말함



## 머신러닝

1. learning rate

> cost value를 떨어트리기 위함
>
> 정해진 값은 없으나 learning rate가 크면 overshoot 현상 발생
>
> cost 함수에서 최소가 되는 때의 W 값을 아는 것이 목적
>
> 현재 W- (learning rate * 기울기) 과정으로 수행하면서 적합한 W를 찾음
>
> cost 값 기준 customizing 필요
>
> 너무 작으면 local minimum 현상 발생

2. 입력 데이터의 pre processing

> feature engineering을 포함해서 각 데이터의 범주와 크기를 살펴봐야
>
> 정규화 `normalizaion`
>
> : x의 최대/최소 값을 가지고 scale 하면 MinmaxScale
>
> 표준화`standardlization`
>
> : 분산과 표준편차를 이용해서 값을 scale 하는 방식

3. `Overfitting`(과적합)

> 모델을 만들어서 학습을 하는데 이것이 학습 데이터에 너무 잘 맞는 모델이 형성됨
>
> 델이 학습데이터에 꼭 맞춰진다면 특수한 현상에 대해 맞춤된 모델이 된 것
>
> 원래 목적은 예측, 실제 데이터를 적용할 때 결과값 예측이 잘 안되는 경우

##### [ 과적합 현상을 피하기 위한 방법 ]

> 1) 많은 training data set이 있어야 함.
>
> ​	적을 수록 과적합 가능
>
> 2) feature(column)의 개수를 가능한 줄임.

4. 학습과정

> 일반적으로 training data set 크기가 굉장히 큼
>
> 따라서 1 epoch를 수행하는 시간이 오래걸림

5.  정확도

> raw data set을 우리가 얻게 되면 training data set, test data set으로 분해(7:3 , 8:2)
>
> : 평가가 이루어져야 모델의 정확도 측정 가능
>
> `n fold cross validation`: 교차 검증 

## Deep Learning

1. 프로그램 설치

> 1) NVDIA에서 제공하는 최신 비디오 드라이버
>
> 2) NVDIA에서 제공하는 GPU를 쓸 수 있도록 만든 라이브러리

2. 새로운 가상환경을 하나 생성 및 전환

> conda create -n gpu_env python=3.6 openssl
>
> activate gpu_env

3. nb_conda 설치

> prompt > conda install nb_conda

4. kernel 설정

> prompt > python -m ipykernerl install --user --name=gpu_env --display-name=[GPU_ENV]

5. install module

> pip install numpy
>
> pip install matplotlib
>
> pip install pandas
>
> 