### CNN

> `Convolutional neural network`
>
> 이미지 학습에 있어 현존하는 가장 효율적인 방법
>
> 이미지 처리에 특화된 알고리즘
>
> 1. 이미지에 대한 특징을 도출해서 labeling -> 신경망 학습
>
> 2. 이미지를 28*28사이즈로 변형 및 필터적용하여 여러번 학습
>
> 3. 포토샵의 선명하게 하는 효과처럼 이미지를 sharp하게 만들어 학습
> 4. or 흑백이미지로 변환하여 학습
> 5. 이러한 방법을 통해 도드라지는 특징을 여러번 학습하는 것이 CNN

`CNN의 필터` 가 늘어나는 만큼 `학습시간`▲

필터+학습시간이 늘어나면 이미지의 크기를 보다 작게 만들어서 효율적인 학습 진행

###### 1장의 컬러사진

: width, height, color(depth: RGB사용) 

: 3차원

###### 여러장의 사진

: 4차원

###### `평면화`

> 실제 이미지 1장은 3차원, 이를 평면화(Flatten) 시켜 1차원으로 표현
>
> 이 때 크기를 조절해야하기 때문에 공간에 대한 데이터 유실할 수 있음
>
> 이런 위험성 때문에 학습/예측 과정 에서 문제발생

###### `CNN` :dress:

> 공간 데이터 유실 위험을 없애고 이미지 특성을 추출해서 학습을 용이하게 하는 방식 -> CNN
>
> CNN은 공간 데이터 유실을 최소화하고 특색 도출 및 한장으로 여러 특색 도출 가능
>
> Deep Learning 과정 이전에 CNN 진행, 그 값을 FC Layer에 넣음



-----

## FC Layer

> `Fully Connected Layer (or Dense Layer)`
>
> 이전 Layer의 모든 node가 다음 Layer의 모든 node에 연결되서 학습되는 구조
>
> 쉽게 말해서 지금까지의 학습은 FC Layer
>
> 이전 Layer의 출력값이 다음 layer입력값으로 사용되었기 때문
>
> : 입력 데이터(1개, 1명의 데이터)가 1차원으로 한정
>
> ∴각각의 이미지가 1차원으로 표현되어야 함!

```
image는 가로*세로*깊이(명암)의 3차원 데이터
FC layer의 특징으로 2차원의 이미지(MNIST)를 1차원으로 변환하여 활용
MNIST는 흑백이미지로 깊이가 불필요, 가로*세로 정보는 784개의 column
MNIST는 간단한 이미지학습/예측 예제
```

##### 이미지 학습의 가장 큰 문제(MNIST제외) :imp:

1. image가 휘거나
2. 각도가 변경되거나
3. 크기가 제각각이거나
4. 변형이 조금만 생겨도 학습이 힘들어짐

------------

## CNN을 위한 용어정리 및 예제

##### EX)

이미지의 크기 32x32x3

필터의 크기 5x5x3

필터의 크기는 설정 가능 하지만 depth는 이미지와 동일한 크기여야 함!

이 때 3은 channel

하나의 이미지를 표현하는 방식으로 생각, Depth 외에 다른 방식이 이미지를 표현한다면 이 또한 channel

컬러사진은 channel 3종류: `green`,`red`,`blue`, * 흑백은 `grey one channel`

[해당개념참고](http://taewan.kim/post/cnn/)



※ 이미지의 유실 가능성이 생길 때

> padding 처리: 원본과 동일한 사이즈로 feature map 출력
>
> 이미지의 중요한 부분은 중앙이기 때문에 padding에 의한 왜곡이 크게 일어나지 않음
>
> padding할 것인지 strides 크기는 스스로 결정

#### --> `Activation Map`

#### `Feature Map`:camera_flash:

> kernel 값과 filter 값을 곱하고 더해서 만듦, 계속 커짐
>
> ∴ relu 함수 적용: 원래 데이터보다 크기를 줄임(너무 크거나 작아지지 않게)
>
> 특징을 뽑아내고 값을 입력값으로 해서 다시 convolution

#### Pooling Layer :closed_lock_with_key:

> 이미지 크기가 너무 큰 경우 사용
>
> stride를 두고 사용하지않고 pooling layer사용하는 것은 stride가 커지면
>
> feature를 잘 뽑아내지 못하게 됨
>
> ```markdown
> ### Pooling Layer 만드는 법
> kernerl, strides 존재
> 곱,합 연산 convolution, feature map
> pooling에서는 mapping
> ```

```markdown
- CNN에서는 대체로 max_pool 사용
- kernel을 activation map에 mapping했을 때 가장 큰 값 하나를 뽑음
- padding사용가능
- 사용X인 경우: size를 원본보다 작게 줄여서 학습에 용이
- 사용하는 경우: 특징을 더욱 도드라지게 뽑아내기 위함
```

